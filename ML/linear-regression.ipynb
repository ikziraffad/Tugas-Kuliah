{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"},"colab":{"name":"linear-regression.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"_7xbUxKiKifR"},"source":["<img src=\"logo.png\" width=\"200\">\n","\n","<center> <h1>Linear Regression with Gradient Descent Using a Made Up Example</h1> </center>\n"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"X32ld-wtKifZ"},"source":["## Gradient descent algorithm\n","From our [video](https://youtu.be/fkS3FkVAPWU) on linear regression, we derived the equation to update the linear model parameters as:    \n","\n","\n","\\begin{equation}\n","\\theta^{+} = \\theta^{-} + \\frac{\\alpha}{m} (y_{i} - h(x_{i}) )\\bar{x}\n","\\end{equation}\n","\n","This minimizes the following cost function\n","\n","\\begin{equation}\n","J(x, \\theta, y) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x_i) - y_i)^2\n","\\end{equation}\n","\n","where\n","\\begin{equation}\n","h(x_i) = \\theta^T \\bar{x}\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"z1W7ZZ70Kifa"},"source":["### Batch gradient descent\n","```FOR j FROM 0 -> max_iteration: \n","    FOR i FROM 0 -> m: \n","        theta += (alpha / m) * (y[i] - h(x[i])) * x_bar\n","    ENDLOOP\n","ENDLOOP\n","```"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"1ZVEyFPXKifc"},"source":["### Stochastic gradient descent\n","```shuffle(x, y)\n","FOR i FROM 0 -> m:\n","    theta += (alpha / m) * (y[i] - h(x[i])) * x_bar  \n","ENDLOOP\n","```"]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":true,"editable":true,"id":"FbGYvfKqKifd"},"source":["import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"dygw3N_AKife"},"source":["\"\"\"Generate data\"\"\"\n","true_slope = 10.889\n","true_intercept = 3.456\n","input_var = np.arange(0.0,100.0)\n","output_var = true_slope * input_var + true_intercept + 500.0 * np.random.rand(len(input_var))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"G3ffRllRKiff"},"source":["%matplotlib notebook\n","plt.figure()\n","plt.scatter(input_var, output_var)\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"Jiqt77P7Kifg"},"source":["def compute_cost(input_var, output_var, params):\n","    \"Compute linear regression cost\"\n","    num_samples = len(input_var)\n","    cost_sum = 0.0\n","    for x,y in zip(input_var, output_var):\n","        y_hat = np.dot(params, np.array([1.0, x]))\n","        cost_sum += (y_hat - y) ** 2\n","    \n","    cost = cost_sum / (num_samples * 2.0)\n","    \n","    return cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"sw1Ag1bgKifh","executionInfo":{"status":"ok","timestamp":1615833855440,"user_tz":-420,"elapsed":1566,"user":{"displayName":"Ikziraffad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}}},"source":["def lin_reg_batch_gradient_descent(input_var, output_var, params, alpha, max_iter):\n","    \"\"\"Compute the params for linear regression using batch gradient descent\"\"\" \n","    iteration = 0\n","    num_samples = len(input_var)\n","    cost = np.zeros(max_iter)\n","    params_store = np.zeros([2, max_iter])\n","    \n","    while iteration < max_iter:\n","        cost[iteration] = compute_cost(input_var, output_var, params)\n","        params_store[:, iteration] = params\n","        \n","        print('--------------------------')\n","        print(f'iteration: {iteration}')\n","        print(f'cost: {cost[iteration]}')\n","        \n","        for x,y in zip(input_var, output_var):\n","            y_hat = np.dot(params, np.array([1.0, x]))\n","            gradient = np.array([1.0, x]) * (y - y_hat)\n","            params += alpha * gradient/num_samples\n","            \n","        iteration += 1\n","    \n","    return params, cost, params_store\n","    "],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"oLaCTaQIKifi"},"source":["\"\"\"Train the model\"\"\"\n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)\n","\n","params_0 = np.array([20.0, 80.0])\n","\n","alpha_batch = 1e-3\n","max_iter = 500\n","params_hat_batch, cost_batch, params_store_batch =\\\n","    lin_reg_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":true,"editable":true,"id":"3h6Qu0VzKifk"},"source":["def lin_reg_stoch_gradient_descent(input_var, output_var, params, alpha):\n","    \"\"\"Compute the params for linear regression using stochastic gradient descent\"\"\"\n","    num_samples = len(input_var)\n","    cost = np.zeros(num_samples)\n","    params_store = np.zeros([2, num_samples])\n","    \n","    i = 0\n","    for x,y in zip(input_var, output_var):\n","        cost[i] = compute_cost(input_var, output_var, params)\n","        params_store[:, i] = params\n","        \n","        print('--------------------------')\n","        print(f'iteration: {i}')\n","        print(f'cost: {cost[i]}')\n","        \n","        y_hat = np.dot(params, np.array([1.0, x]))\n","        gradient = np.array([1.0, x]) * (y - y_hat)\n","        params += alpha * gradient/num_samples\n","        \n","        i += 1\n","            \n","    return params, cost, params_store"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"6sHPNF5nKifl"},"source":["alpha = 1e-3\n","params_0 = np.array([20.0, 80.0])\n","params_hat, cost, params_store =\\\n","lin_reg_stoch_gradient_descent(x_train, y_train, params_0, alpha)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"Fws6wfYLKifl"},"source":["plt.figure()\n","plt.scatter(x_test, y_test)\n","plt.plot(x_test, params_hat_batch[0] + params_hat_batch[1]*x_test, 'g', label='batch')\n","plt.plot(x_test, params_hat[0] + params_hat[1]*x_test, '-r', label='stochastic')\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.legend()\n","plt.show()\n","print(f'batch      T0, T1: {params_hat_batch[0]}, {params_hat_batch[1]}')\n","print(f'stochastic T0, T1: {params_hat[0]}, {params_hat[1]}')\n","rms_batch = np.sqrt(np.mean(np.square(params_hat_batch[0] + params_hat_batch[1]*x_test - y_test)))\n","rms_stochastic = np.sqrt(np.mean(np.square(params_hat[0] + params_hat[1]*x_test - y_test)))\n","print(f'batch rms:      {rms_batch}')\n","print(f'stochastic rms: {rms_stochastic}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"_uY-OEq-Kifm"},"source":["plt.figure()\n","plt.plot(np.arange(max_iter), cost_batch, 'r', label='batch')\n","plt.plot(np.arange(len(cost)), cost, 'g', label='stochastic')\n","plt.xlabel('iteration')\n","plt.ylabel('normalized cost')\n","plt.legend()\n","plt.show()\n","print(f'min cost with BGD: {np.min(cost_batch)}')\n","print(f'min cost with SGD: {np.min(cost)}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":true,"editable":true,"id":"z0xEGMO2Kifm"},"source":[""],"execution_count":null,"outputs":[]}]}