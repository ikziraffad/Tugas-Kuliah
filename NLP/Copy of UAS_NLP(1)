{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of UAS_NLP","provenance":[{"file_id":"1KbsVdStXixHA6siZiyIOZH22lltiKXGF","timestamp":1627309774209}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"_4_sHTdjFQgD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627310097037,"user_tz":-420,"elapsed":3416,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"014c5898-6425-44af-8619-7c2638b9828e"},"source":["pip install nltk\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4QSpaWzuFAxK","executionInfo":{"status":"ok","timestamp":1627310097583,"user_tz":-420,"elapsed":549,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}}},"source":["import nltk\n","import numpy as np\n","import pandas as pd"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"rGoLO92Y1BAE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627310098024,"user_tz":-420,"elapsed":445,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"61edfb4b-2310-4fdf-898d-ff0d5cbf7641"},"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"Rq-HeT8BOBh-","executionInfo":{"status":"ok","timestamp":1627310098025,"user_tz":-420,"elapsed":7,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}}},"source":["dataset={'Data':[\"I feel like I am drowning. #depression #anxiety #failure #worthless\",\n","                 \"#panic Panic attack from fear of starting new medication\",\n","                 \"My bus was in a car crash... I'm still shaking a bit... This week was an absolute horror and this was the icing on the cake... #terrible\",\n","                 \"Just got back from seeing @GaryDelaney in Burslem. AMAZING!! Face still hurts from laughing so much #hilarious\",\n","                 \"It's the #FirstDayofFall and I'm so happy. Sipping my #PumpkinSpice flavoured coffee and #smiling! Happy Fall everyone! #amwriting\",\n","                 \"Morning all! Of course it is sunny on this Monday morning to cheerfully welcome us back to work.:)\"],\n","         'Label':['fear','fear','fear','joy','joy','joy']\n","         }\n","\n","ds =pd.DataFrame(dataset)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"OsuM5mc3mao4","executionInfo":{"status":"ok","timestamp":1627310098026,"user_tz":-420,"elapsed":7,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}}},"source":["from nltk import punkt\n","from nltk.corpus import stopwords\n","import pandas as pd\n","ds = ds.applymap(str.lower)\n","#remove all number\n","ds['Data'] = ds['Data'].str.replace(r'\\d+','')\n","#remove all punctuation\n","ds['Data'] = ds['Data'].str.replace('[^\\w\\s]','')\n","#Remove WhiteSpaces\n","ds['Data'] = ds['Data'].str.strip()\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AAw5a1WvdSnV"},"source":["We need to remove all the lowering the data helps to make the dataset become consistent. We also need to remove number and punctuation as it isn't relevant. Also we remove whitespaces to remove trailing and ending space."]},{"cell_type":"markdown","metadata":{"id":"obTGIhV40JW4"},"source":["Tokenization and Stopword removal"]},{"cell_type":"code","metadata":{"id":"B53_hIq_xGr2","colab":{"base_uri":"https://localhost:8080/","height":234},"executionInfo":{"status":"ok","timestamp":1627310098027,"user_tz":-420,"elapsed":8,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"7c351a5e-9147-4ace-e228-33f9103816be"},"source":["stop_words = set(stopwords.words('english'))\n","ds['Edit'] = ds['Data'].apply(nltk.word_tokenize)\n","\n","ds['Edit']= ds['Edit'].apply(lambda x: [i for i in x if i not in stop_words])\n","ds"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Data</th>\n","      <th>Label</th>\n","      <th>Edit</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>i feel like i am drowning depression anxiety f...</td>\n","      <td>fear</td>\n","      <td>[feel, like, drowning, depression, anxiety, fa...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>panic panic attack from fear of starting new m...</td>\n","      <td>fear</td>\n","      <td>[panic, panic, attack, fear, starting, new, me...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>my bus was in a car crash im still shaking a b...</td>\n","      <td>fear</td>\n","      <td>[bus, car, crash, im, still, shaking, bit, wee...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>just got back from seeing garydelaney in bursl...</td>\n","      <td>joy</td>\n","      <td>[got, back, seeing, garydelaney, burslem, amaz...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>its the firstdayoffall and im so happy sipping...</td>\n","      <td>joy</td>\n","      <td>[firstdayoffall, im, happy, sipping, pumpkinsp...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>morning all of course it is sunny on this mond...</td>\n","      <td>joy</td>\n","      <td>[morning, course, sunny, monday, morning, chee...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Data  ...                                               Edit\n","0  i feel like i am drowning depression anxiety f...  ...  [feel, like, drowning, depression, anxiety, fa...\n","1  panic panic attack from fear of starting new m...  ...  [panic, panic, attack, fear, starting, new, me...\n","2  my bus was in a car crash im still shaking a b...  ...  [bus, car, crash, im, still, shaking, bit, wee...\n","3  just got back from seeing garydelaney in bursl...  ...  [got, back, seeing, garydelaney, burslem, amaz...\n","4  its the firstdayoffall and im so happy sipping...  ...  [firstdayoffall, im, happy, sipping, pumpkinsp...\n","5  morning all of course it is sunny on this mond...  ...  [morning, course, sunny, monday, morning, chee...\n","\n","[6 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"LqtNPjovgko1"},"source":["We use tokenize to split the text into smaller pieces because the common way to process the text happens at the token level and we remove stopwords as it have no important meaning. All of these changes are saved into a new column of dataset called edit, so we can see the diffrent after the preprocessing of data."]},{"cell_type":"markdown","metadata":{"id":"0vTW63XYInH3"},"source":["Changing into Base form"]},{"cell_type":"code","metadata":{"id":"d9XZveajImXy","colab":{"base_uri":"https://localhost:8080/","height":234},"executionInfo":{"status":"ok","timestamp":1627310099919,"user_tz":-420,"elapsed":1899,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"33faec5c-9178-400d-a576-55959a1b5414"},"source":["\n","#lemmatization\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","ds['Edit'] = ds['Edit'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n","ds\n","\n"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Data</th>\n","      <th>Label</th>\n","      <th>Edit</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>i feel like i am drowning depression anxiety f...</td>\n","      <td>fear</td>\n","      <td>[feel, like, drowning, depression, anxiety, fa...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>panic panic attack from fear of starting new m...</td>\n","      <td>fear</td>\n","      <td>[panic, panic, attack, fear, starting, new, me...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>my bus was in a car crash im still shaking a b...</td>\n","      <td>fear</td>\n","      <td>[bus, car, crash, im, still, shaking, bit, wee...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>just got back from seeing garydelaney in bursl...</td>\n","      <td>joy</td>\n","      <td>[got, back, seeing, garydelaney, burslem, amaz...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>its the firstdayoffall and im so happy sipping...</td>\n","      <td>joy</td>\n","      <td>[firstdayoffall, im, happy, sipping, pumpkinsp...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>morning all of course it is sunny on this mond...</td>\n","      <td>joy</td>\n","      <td>[morning, course, sunny, monday, morning, chee...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Data  ...                                               Edit\n","0  i feel like i am drowning depression anxiety f...  ...  [feel, like, drowning, depression, anxiety, fa...\n","1  panic panic attack from fear of starting new m...  ...  [panic, panic, attack, fear, starting, new, me...\n","2  my bus was in a car crash im still shaking a b...  ...  [bus, car, crash, im, still, shaking, bit, wee...\n","3  just got back from seeing garydelaney in bursl...  ...  [got, back, seeing, garydelaney, burslem, amaz...\n","4  its the firstdayoffall and im so happy sipping...  ...  [firstdayoffall, im, happy, sipping, pumpkinsp...\n","5  morning all of course it is sunny on this mond...  ...  [morning, course, sunny, monday, morning, chee...\n","\n","[6 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"K3aK2nsrhzwZ"},"source":["Using lemmatization we change the input into their base form of word becaise we need to extract the data from the text and insert it into a database."]},{"cell_type":"markdown","metadata":{"id":"5VD4RG4mku50"},"source":["Changing the label form positive and negative into 1 and 0 as it is easier to predict a number."]},{"cell_type":"code","metadata":{"id":"yGR45veu3RfO","executionInfo":{"status":"ok","timestamp":1627310099921,"user_tz":-420,"elapsed":23,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}}},"source":["filter = ds['Label'] == 'positive'\n","dfpos = ds[filter]\n","dfneg = ds[~filter]"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRWNFiLr3VM4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627310099922,"user_tz":-420,"elapsed":23,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"082ec89a-9a58-4aea-dced-b7b8f50bedb2"},"source":["dfpos = dfpos['Edit']\n","dfpos"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Series([], Name: Edit, dtype: object)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"xjndSZ2M5_YY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627310099923,"user_tz":-420,"elapsed":22,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"08e28267-c84c-4dbc-f17c-b04edccc0a0c"},"source":["dfneg = dfneg['Edit']\n","dfneg"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    [feel, like, drowning, depression, anxiety, fa...\n","1    [panic, panic, attack, fear, starting, new, me...\n","2    [bus, car, crash, im, still, shaking, bit, wee...\n","3    [got, back, seeing, garydelaney, burslem, amaz...\n","4    [firstdayoffall, im, happy, sipping, pumpkinsp...\n","5    [morning, course, sunny, monday, morning, chee...\n","Name: Edit, dtype: object"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"dZTbAzqKkvY2","executionInfo":{"status":"ok","timestamp":1627310099924,"user_tz":-420,"elapsed":20,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}}},"source":["for i in range(len(ds['Label'])) :\n","  if ds['Label'][i] == 'positive':\n","    ds['Label'][i]=1\n","  else :\n","    ds['Label'][i]=0"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Cnbg7fQlCPA","colab":{"base_uri":"https://localhost:8080/","height":234},"executionInfo":{"status":"ok","timestamp":1627310099925,"user_tz":-420,"elapsed":21,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"b7aaf1be-7003-4d02-fd54-12e2a0d82cf3"},"source":["ds"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Data</th>\n","      <th>Label</th>\n","      <th>Edit</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>i feel like i am drowning depression anxiety f...</td>\n","      <td>0</td>\n","      <td>[feel, like, drowning, depression, anxiety, fa...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>panic panic attack from fear of starting new m...</td>\n","      <td>0</td>\n","      <td>[panic, panic, attack, fear, starting, new, me...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>my bus was in a car crash im still shaking a b...</td>\n","      <td>0</td>\n","      <td>[bus, car, crash, im, still, shaking, bit, wee...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>just got back from seeing garydelaney in bursl...</td>\n","      <td>0</td>\n","      <td>[got, back, seeing, garydelaney, burslem, amaz...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>its the firstdayoffall and im so happy sipping...</td>\n","      <td>0</td>\n","      <td>[firstdayoffall, im, happy, sipping, pumpkinsp...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>morning all of course it is sunny on this mond...</td>\n","      <td>0</td>\n","      <td>[morning, course, sunny, monday, morning, chee...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Data  ...                                               Edit\n","0  i feel like i am drowning depression anxiety f...  ...  [feel, like, drowning, depression, anxiety, fa...\n","1  panic panic attack from fear of starting new m...  ...  [panic, panic, attack, fear, starting, new, me...\n","2  my bus was in a car crash im still shaking a b...  ...  [bus, car, crash, im, still, shaking, bit, wee...\n","3  just got back from seeing garydelaney in bursl...  ...  [got, back, seeing, garydelaney, burslem, amaz...\n","4  its the firstdayoffall and im so happy sipping...  ...  [firstdayoffall, im, happy, sipping, pumpkinsp...\n","5  morning all of course it is sunny on this mond...  ...  [morning, course, sunny, monday, morning, chee...\n","\n","[6 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"3OeZcUhCksup"},"source":["Taking all of the word and put it into one place"]},{"cell_type":"code","metadata":{"id":"La1cO_ZC61pX","executionInfo":{"status":"ok","timestamp":1627310099926,"user_tz":-420,"elapsed":20,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}}},"source":["def get_all_words(dataset):\n","    allword=[]\n","    for i in dataset:\n","        for y in i:\n","            allword.append(y)\n","    return allword\n","    \n","all_pos_words = get_all_words(dfpos)\n","all_neg_words = get_all_words(dfneg)\n","allword= get_all_words(ds['Edit'])\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJtmsfVpIfbu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627310099926,"user_tz":-420,"elapsed":19,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"dbce66f9-5dae-4e8a-8a71-67d489bc3cff"},"source":["allword"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['feel',\n"," 'like',\n"," 'drowning',\n"," 'depression',\n"," 'anxiety',\n"," 'failure',\n"," 'worthless',\n"," 'panic',\n"," 'panic',\n"," 'attack',\n"," 'fear',\n"," 'starting',\n"," 'new',\n"," 'medication',\n"," 'bus',\n"," 'car',\n"," 'crash',\n"," 'im',\n"," 'still',\n"," 'shaking',\n"," 'bit',\n"," 'week',\n"," 'absolute',\n"," 'horror',\n"," 'icing',\n"," 'cake',\n"," 'terrible',\n"," 'got',\n"," 'back',\n"," 'seeing',\n"," 'garydelaney',\n"," 'burslem',\n"," 'amazing',\n"," 'face',\n"," 'still',\n"," 'hurt',\n"," 'laughing',\n"," 'much',\n"," 'hilarious',\n"," 'firstdayoffall',\n"," 'im',\n"," 'happy',\n"," 'sipping',\n"," 'pumpkinspice',\n"," 'flavoured',\n"," 'coffee',\n"," 'smiling',\n"," 'happy',\n"," 'fall',\n"," 'everyone',\n"," 'amwriting',\n"," 'morning',\n"," 'course',\n"," 'sunny',\n"," 'monday',\n"," 'morning',\n"," 'cheerfully',\n"," 'welcome',\n"," 'u',\n"," 'back',\n"," 'work']"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"G7TTSUp5lKYc"},"source":["Preparing the data using word2vec"]},{"cell_type":"code","metadata":{"id":"lges3-XfRLHH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627310100423,"user_tz":-420,"elapsed":513,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"b0ff5b99-9018-4238-a72d-d46ed68ba926"},"source":["from gensim.models import Word2Vec \n","\n","# Create CBOW model \n","model = Word2Vec([allword], min_count = 1, size = 100, window = 5)\n","#setup the parameter of the model one by1\n","print(model.wv.vocab)\n","print(model.wv.syn0)\n","print(model.wv.syn0.shape)\n","\n","filename=\"word2vecembedding.txt\"\n","model.wv.save_word2vec_format(filename,binary=False)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["{'feel': <gensim.models.keyedvectors.Vocab object at 0x7f1b73925a10>, 'like': <gensim.models.keyedvectors.Vocab object at 0x7f1b73925b90>, 'drowning': <gensim.models.keyedvectors.Vocab object at 0x7f1b73925c50>, 'depression': <gensim.models.keyedvectors.Vocab object at 0x7f1b73917050>, 'anxiety': <gensim.models.keyedvectors.Vocab object at 0x7f1b711ccb90>, 'failure': <gensim.models.keyedvectors.Vocab object at 0x7f1b711ecf90>, 'worthless': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175450>, 'panic': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175410>, 'attack': <gensim.models.keyedvectors.Vocab object at 0x7f1b7145cc10>, 'fear': <gensim.models.keyedvectors.Vocab object at 0x7f1b711ccbd0>, 'starting': <gensim.models.keyedvectors.Vocab object at 0x7f1b711754d0>, 'new': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175510>, 'medication': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175550>, 'bus': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175590>, 'car': <gensim.models.keyedvectors.Vocab object at 0x7f1b711755d0>, 'crash': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175610>, 'im': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175650>, 'still': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175690>, 'shaking': <gensim.models.keyedvectors.Vocab object at 0x7f1b711756d0>, 'bit': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175710>, 'week': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175750>, 'absolute': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175790>, 'horror': <gensim.models.keyedvectors.Vocab object at 0x7f1b711757d0>, 'icing': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175810>, 'cake': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175850>, 'terrible': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175890>, 'got': <gensim.models.keyedvectors.Vocab object at 0x7f1b711758d0>, 'back': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175910>, 'seeing': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175950>, 'garydelaney': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175990>, 'burslem': <gensim.models.keyedvectors.Vocab object at 0x7f1b711759d0>, 'amazing': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175a10>, 'face': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175a50>, 'hurt': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175a90>, 'laughing': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175ad0>, 'much': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175b10>, 'hilarious': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175b50>, 'firstdayoffall': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175b90>, 'happy': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175bd0>, 'sipping': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175c10>, 'pumpkinspice': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175c50>, 'flavoured': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175c90>, 'coffee': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175cd0>, 'smiling': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175d10>, 'fall': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175d50>, 'everyone': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175d90>, 'amwriting': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175dd0>, 'morning': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175e10>, 'course': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175e50>, 'sunny': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175e90>, 'monday': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175ed0>, 'cheerfully': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175f10>, 'welcome': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175f50>, 'u': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175f90>, 'work': <gensim.models.keyedvectors.Vocab object at 0x7f1b71175fd0>}\n","[[-2.7581463e-03 -6.2548657e-05  2.9189086e-03 ... -3.4847090e-03\n","   1.4377780e-03 -6.2647072e-04]\n"," [ 2.9073711e-03 -2.5150517e-03  4.5606429e-03 ...  1.9125515e-03\n","  -1.0182267e-03  2.6949514e-03]\n"," [-2.1022900e-03 -1.8261799e-04  4.2979090e-04 ...  2.9635094e-03\n","  -1.0360277e-03 -8.9333416e-04]\n"," ...\n"," [ 2.0722332e-03 -4.6193288e-04  1.1812199e-03 ...  2.9331198e-04\n","   3.8784707e-03 -4.1090692e-03]\n"," [ 4.8473137e-03 -1.2258868e-04 -1.7274183e-03 ... -3.7516512e-03\n","   1.4847647e-03  4.1535753e-03]\n"," [ 3.3363295e-03 -3.7206258e-03  1.1532976e-03 ...  3.7461685e-03\n","  -3.3622500e-03 -1.9398063e-03]]\n","(55, 100)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n","  import sys\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n","  \n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"1hylv-V2mR6P"},"source":["Word2vec is a two layer neural network that is trained to change word into vector so that the words can be understand by a deep learning model.\n","  From a large corpus of word it will produce a vector space, with each unique  word in a corpus being assign a vector in the space.\n","  Using a neural network word2vec take input as one-hot vectors, which is a vector with the same length, filled with zero excepet the index that represent the word we want to repesent, assign \"1\".\n","\n","the rows of hidden layer weight matrix, are actually the word vector, it operates as a lookup table. The output, is just a word vector of the input word.\n","\n","and the output layer is just a softmax activation function.\n"]},{"cell_type":"code","metadata":{"id":"ahC5PxfoN0GX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627310100424,"user_tz":-420,"elapsed":15,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"610e3cfb-848f-479b-b387-db1e85e40b5f"},"source":["model.wv.most_similar('anxiety')"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('back', 0.23617756366729736),\n"," ('shaking', 0.1691039502620697),\n"," ('terrible', 0.15265919268131256),\n"," ('medication', 0.1364797055721283),\n"," ('face', 0.12278386205434799),\n"," ('happy', 0.09245570003986359),\n"," ('sunny', 0.09151391685009003),\n"," ('got', 0.07983562350273132),\n"," ('crash', 0.07938191294670105),\n"," ('like', 0.07765959203243256)]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"KKPFbZRsqrs7"},"source":["After we change the word into vector it become somekind of data of words that's saved into \"word2vecembedding.txt\""]},{"cell_type":"markdown","metadata":{"id":"yTdd8AyOrZDp"},"source":["We need to read the \"word2vecembedding.txt\" and take the value,word, and the index."]},{"cell_type":"code","metadata":{"id":"sKunko1s4PwN","executionInfo":{"status":"ok","timestamp":1627310100426,"user_tz":-420,"elapsed":8,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}}},"source":["import os\n","embeddings_index = {}\n","f= open(os.path.join('','word2vecembedding.txt'),encoding=\"utf-8\")\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs= np.asarray(values[1:])\n","  embeddings_index[word]=coefs\n","f.close()"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"90v0GESNr2i0"},"source":["Using keras we take tokenzier and fit the transformed dataset and using pad_sequences we pad the vector so that all of the input have the same width of vector."]},{"cell_type":"code","metadata":{"id":"g3FcXBA0VDqw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627310102237,"user_tz":-420,"elapsed":1818,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"65fbd200-d5d7-4789-ad0e-0ec398934a94"},"source":["from tensorflow.python.keras.preprocessing.text import Tokenizer\n","from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n","\n","tokenizer_obj = Tokenizer()\n","tokenizer_obj.fit_on_texts(ds['Edit'])\n","total_reviews = ds['Edit'].values\n","\n","sequences = tokenizer_obj.texts_to_sequences(ds['Edit'])\n","\n","max_length = max([len(i) for i in total_reviews])\n","print(max_length)\n","\n","\n","#pad\n","word_index= tokenizer_obj.word_index\n","review_pad = pad_sequences(sequences,maxlen=max_length)\n","sentiment = ds['Label'].values\n","print(review_pad)\n","print(sentiment.shape)\n","\n","#define vocab size\n","vocab_size = len(tokenizer_obj.word_index)+1\n","print(vocab_size)\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["13\n","[[ 0  0  0  0  0  0  7  8  9 10 11 12 13]\n"," [ 0  0  0  0  0  0  1  1 14 15 16 17 18]\n"," [19 20 21  2  3 22 23 24 25 26 27 28 29]\n"," [ 0 30  4 31 32 33 34 35  3 36 37 38 39]\n"," [ 0 40  2  5 41 42 43 44 45  5 46 47 48]\n"," [ 0  0  0  6 49 50 51  6 52 53 54  4 55]]\n","(6,)\n","56\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s73Qkr2JtDe3"},"source":["Form all of the matrix above, we transform the word into vector form the word2vec data by picking the corresponding index. "]},{"cell_type":"code","metadata":{"id":"K_hh2_HduzMX","executionInfo":{"status":"ok","timestamp":1627310102238,"user_tz":-420,"elapsed":32,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}}},"source":["num_words = len(word_index)+1\n","embedding_matrix = np.zeros((num_words,100))\n","for word, i in word_index.items():\n","  if i>num_words:\n","    continue\n","  embedding_vector = embeddings_index.get(word)\n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"C6M8UPgNvihk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627310102239,"user_tz":-420,"elapsed":30,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"5c019ff5-6f67-4bde-ff91-95299661e80f"},"source":["print(num_words)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["56\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_dEQTjZyZ1Vu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627310105201,"user_tz":-420,"elapsed":2973,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"f7909773-42c6-405e-f46a-919228fe8b15"},"source":["pip install keras_metrics"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras_metrics in /usr/local/lib/python3.7/dist-packages (1.1.0)\n","Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.7/dist-packages (from keras_metrics) (2.4.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras>=2.1.5->keras_metrics) (3.13)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from Keras>=2.1.5->keras_metrics) (1.19.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras>=2.1.5->keras_metrics) (3.1.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras>=2.1.5->keras_metrics) (1.4.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->Keras>=2.1.5->keras_metrics) (1.5.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KPQX95P0uCgB"},"source":["These are the BPNN model using keras, in keras the model automaticly use BPNN so we only need to specify the model, the input layer consist of how many the length is and for the hidden layer we use 32 hidden layer. And 1 dense layer of the output."]},{"cell_type":"code","metadata":{"id":"xf3WIXm-e3mn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627310105690,"user_tz":-420,"elapsed":492,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"d8fb9cd1-1726-4802-ab1d-8abe4b1be4ee"},"source":["from keras import Sequential\n","from keras.layers import Dense,Embedding,LSTM,GRU\n","from keras.layers.embeddings import Embedding\n","import keras_metrics\n","\n","EMBEDDING_DIM = 100\n","\n","model= Sequential()\n","model.add(Embedding(vocab_size,EMBEDDING_DIM,input_length=max_length))\n","model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',keras_metrics.precision(), keras_metrics.recall(),keras_metrics.f1_score()])\n","\n","model.summary()"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 13, 100)           5600      \n","_________________________________________________________________\n","gru (GRU)                    (None, 32)                12864     \n","_________________________________________________________________\n","dense (Dense)                (None, 1)                 33        \n","=================================================================\n","Total params: 18,497\n","Trainable params: 18,497\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MbPr-QOUubyn"},"source":["We split the data into 4 training and 2 testing, so that we can see if the model could project / see if the model could see if the model could predict negative and positive."]},{"cell_type":"code","metadata":{"id":"CHKVgY2Z_pqL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627310105691,"user_tz":-420,"elapsed":7,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"57d0923f-e972-4871-ceb6-0e62ccc14509"},"source":["VALIDATION_SPLIT = 0.4\n","indices=np.arange(review_pad.shape[0])\n","\n","review_pad = review_pad[indices]\n","sentiment = sentiment[indices]\n","num_validation_samples = int (VALIDATION_SPLIT*review_pad.shape[0])\n","x_train_pad = review_pad[:-num_validation_samples]\n","y_train = sentiment[:-num_validation_samples]\n","x_test_pad = review_pad[-num_validation_samples:]\n","y_test = sentiment[-num_validation_samples:]\n","\n","print(x_train_pad.shape)\n","print(y_train.shape)\n","print(x_test_pad.shape)\n","print(y_test.shape)\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["(4, 13)\n","(4,)\n","(2, 13)\n","(2,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uBEoMKMavGoI"},"source":["These are the training model"]},{"cell_type":"code","metadata":{"id":"EnRQz_bOx8VT","colab":{"base_uri":"https://localhost:8080/","height":303},"executionInfo":{"status":"error","timestamp":1627310106405,"user_tz":-420,"elapsed":718,"user":{"displayName":"Daffa Rizki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiO8ZF06hlqkKJVtO2Rm1cBzF37XQAufwXDgzK0Tg=s64","userId":"12805345379547597214"}},"outputId":"4a0b2b3f-1d72-49f5-b575-96ea64b108f8"},"source":["themodel = model.fit(x_train_pad,y_train, epochs= 10, validation_data=(x_test_pad,y_test),verbose=2)"],"execution_count":24,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-8091ae68a038>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mthemodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_pad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_pad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1120\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1346\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m                **kwargs):\n\u001b[1;32m    230\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    233\u001b[0m         sample_weights, sample_weight_modes)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_is_scipy_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1429\u001b[0m   \"\"\"\n\u001b[1;32m   1430\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1431\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1439\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."]}]},{"cell_type":"markdown","metadata":{"id":"u23fuF_jvQNV"},"source":["At the last one, We got accuracy of 1.000(100%) , recall of 1.000(100%) and f1_score of 1.000(100%), This score are good enough, because the dataset only contain of 6 data and from that 6 data there's only 4 data for trainning."]},{"cell_type":"markdown","metadata":{"id":"YLkts7dkFxjk"},"source":["https://medium.com/@ksnugroho/dasar-text-preprocessing-dengan-python-a4fa52608ffe\n","\n","https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n","\n","https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n","\n","https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n","\n","https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n","\n","https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\n","\n","https://israelg99.github.io/2017-03-23-Word2Vec-Explained/\n","\n"]}]}