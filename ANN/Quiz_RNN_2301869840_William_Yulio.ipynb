{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Quiz_RNN_2301869840_William_Yulio.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"KTL7EPYIkBrw"},"source":["import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMBRCGrRjPCU"},"source":["np.random.seed(1)\n","sin_wave = np.array([math.sin(x) for x in np.arange(200)])\n","\n","X = []\n","Y = []\n","\n","seq_len = 10\n","num_records = len(sin_wave) - seq_len\n","\n","for i in range(50):\n","    X.append(sin_wave[i:i+seq_len])\n","\n","    \n","X = np.array(X)\n","X = np.expand_dims(X, axis=2)\n","\n","Y = np.random.randint(2, size=50)\n","Y = np.array(Y)\n","Y = np.expand_dims(Y, axis=1)\n","\n","X_train =  X[:30]\n","Y_train =  Y[:30]\n","X_test = X[30:]\n","Y_test = Y[30:]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a3bZ7v6UmP04"},"source":["#RNN Architecture\n","\n","learning_rate = 0.05\n","nepoch = 20               \n","T = seq_len                   # length of sequence\n","hidden_dim = 10         \n","output_dim = 1\n","\n","# bptt_truncate = 0\n","# min_clip_value = -10\n","# max_clip_value = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QayLowxFmPua","outputId":"ca860e8d-3cba-4dd4-e130-8b3dfe50adcf"},"source":["#randomly initialize weights\n","\n","U = np.random.uniform(0, 1, (hidden_dim, T))\n","W = np.random.uniform(0, 1, (hidden_dim, hidden_dim))\n","V = np.random.uniform(0, 1, (output_dim, hidden_dim))\n","print(U)\n","print('\\n')\n","print(V)\n","print('\\n')\n","print(W)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.89460666 0.08504421 0.03905478 0.16983042 0.8781425  0.09834683\n","  0.42110763 0.95788953 0.53316528 0.69187711]\n"," [0.31551563 0.68650093 0.83462567 0.01828828 0.75014431 0.98886109\n","  0.74816565 0.28044399 0.78927933 0.10322601]\n"," [0.44789353 0.9085955  0.29361415 0.28777534 0.13002857 0.01936696\n","  0.67883553 0.21162812 0.26554666 0.49157316]\n"," [0.05336255 0.57411761 0.14672857 0.58930554 0.69975836 0.10233443\n","  0.41405599 0.69440016 0.41417927 0.04995346]\n"," [0.53589641 0.66379465 0.51488911 0.94459476 0.58655504 0.90340192\n","  0.1374747  0.13927635 0.80739129 0.39767684]\n"," [0.1653542  0.92750858 0.34776586 0.7508121  0.72599799 0.88330609\n","  0.62367221 0.75094243 0.34889834 0.26992789]\n"," [0.89588622 0.42809119 0.96484005 0.6634415  0.62169572 0.11474597\n","  0.94948926 0.44991213 0.57838961 0.4081368 ]\n"," [0.23702698 0.90337952 0.57367949 0.00287033 0.61714491 0.3266449\n","  0.5270581  0.8859421  0.35726976 0.90853515]\n"," [0.62336012 0.01582124 0.92943723 0.69089692 0.99732285 0.17234051\n","  0.13713575 0.93259546 0.69681816 0.06600017]\n"," [0.75546305 0.75387619 0.92302454 0.71152476 0.12427096 0.01988013\n","  0.02621099 0.02830649 0.24621107 0.86002795]]\n","\n","\n","[[0.94010748 0.58201418 0.87883198 0.84473445 0.90539232 0.45988027\n","  0.54634682 0.79860359 0.28571885 0.49025352]]\n","\n","\n","[[0.53883106 0.55282198 0.84203089 0.12417332 0.27918368 0.58575927\n","  0.96959575 0.56103022 0.01864729 0.80063267]\n"," [0.23297427 0.8071052  0.38786064 0.86354185 0.74712164 0.55624023\n","  0.13645523 0.05991769 0.12134346 0.04455188]\n"," [0.10749413 0.22570934 0.71298898 0.55971698 0.01255598 0.07197428\n","  0.96727633 0.56810046 0.20329323 0.25232574]\n"," [0.74382585 0.19542948 0.58135893 0.97001999 0.8468288  0.23984776\n","  0.49376971 0.61995572 0.8289809  0.15679139]\n"," [0.0185762  0.07002214 0.48634511 0.60632946 0.56885144 0.31736241\n","  0.98861615 0.57974522 0.38014117 0.55094822]\n"," [0.74533443 0.66923289 0.26491956 0.06633483 0.3700842  0.62971751\n","  0.21017401 0.75275555 0.06653648 0.2603151 ]\n"," [0.80475456 0.19343428 0.63946088 0.52467031 0.92480797 0.26329677\n","  0.06596109 0.73506596 0.77217803 0.90781585]\n"," [0.93197207 0.01395157 0.23436209 0.61677836 0.94901632 0.95017612\n","  0.55665319 0.91560635 0.64156621 0.39000771]\n"," [0.48599067 0.60431048 0.54954792 0.92618143 0.91873344 0.39487561\n","  0.96326253 0.17395567 0.12632952 0.13507916]\n"," [0.50566217 0.02152481 0.94797021 0.82711547 0.01501898 0.17619626\n","  0.33206357 0.13099684 0.80949069 0.34473665]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DiwTCtM7mPSO","outputId":"53d29f5f-664f-480e-fad4-a9bda9db25eb"},"source":["#forward pass\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","for epoch in range(nepoch):\n","    # check loss on train\n","    loss = 0.0\n","    \n","    # do a forward pass to get prediction\n","    for i in range(Y.shape[0]):\n","        x, y = X[i], Y[i]                    # get input, output values of each record\n","        prev_s = np.zeros((hidden_dim, 1))   # here, prev-s is the value of the previous activation of hidden layer; which is initialized as all zeroes\n","        for t in range(T):\n","            new_input = np.zeros(x.shape)    # we then do a forward pass for every timestep in the sequence\n","            new_input[t] = x[t]              # for this, we define a single input for that timestep\n","            mulu = np.dot(U, new_input)\n","            mulw = np.dot(W, prev_s)\n","            add = mulw + mulu\n","            s = sigmoid(add)\n","            mulv = sigmoid(np.dot(V, s))\n","            prev_s = s\n","\n","    # calculate error \n","        loss_per_record = (y - mulv)**2 / 2\n","        loss += loss_per_record\n","    loss = loss / float(y.shape[0])\n","\n","    # # check loss on val\n","    # val_loss = 0.0\n","    # for i in range(Y_val.shape[0]):\n","    #     x, y = X_val[i], Y_val[i]\n","    #     prev_s = np.zeros((hidden_dim, 1))\n","    #     for t in range(T):\n","    #         new_input = np.zeros(x.shape)\n","    #         new_input[t] = x[t]\n","    #         mulu = np.dot(U, new_input)\n","    #         mulw = np.dot(W, prev_s)\n","    #         add = mulw + mulu\n","    #         s = sigmoid(add)\n","    #         mulv = sigmoid(np.dot(V, s))\n","    #         prev_s = s\n","\n","    #     loss_per_record = (y - mulv)**2 / 2\n","    #     val_loss += loss_per_record\n","    # val_loss = val_loss / float(y.shape[0])\n","\n","    print('Epoch: ', epoch + 1, ', Loss: ', loss)\n","\n","    # train model\n","    for i in range(Y.shape[0]):\n","        x, y = X[i], Y[i]\n","    \n","        layers = []\n","        prev_s = np.zeros((hidden_dim, 1))\n","        dU = np.zeros(U.shape)\n","        dV = np.zeros(V.shape)\n","        dW = np.zeros(W.shape)\n","        \n","        dU_t = np.zeros(U.shape)\n","        dV_t = np.zeros(V.shape)\n","        dW_t = np.zeros(W.shape)\n","        \n","        dU_i = np.zeros(U.shape)\n","        dW_i = np.zeros(W.shape)\n","        \n","        # forward pass\n","        for t in range(T):\n","            new_input = np.zeros(x.shape)\n","            new_input[t] = x[t]\n","            mulu = np.dot(U, new_input)\n","            mulw = np.dot(W, prev_s)\n","            add = mulw + mulu\n","            s = sigmoid(add)\n","            mulv = sigmoid(np.dot(V, s))\n","            layers.append({'s':s, 'prev_s':prev_s})\n","            prev_s = s\n","\n","        # derivative of pred\n","        dmulv = (mulv - y)*(1-mulv)*(mulv)\n","        \n","        # backward pass\n","        for t in range(T):\n","            dV_t = np.dot(dmulv, np.transpose(layers[t]['s']))\n","            dsv = np.dot(np.transpose(V), dmulv)\n","            \n","            ds = dsv\n","            dadd = add * (1 - add) * ds\n","            \n","            dmulw = dadd * np.ones_like(mulw)\n","\n","            dprev_s = np.dot(np.transpose(W), dmulw)\n","\n","\n","            for i in range(t, 0, -1):\n","                ds = dsv + dprev_s\n","                dadd = add * (1 - add) * ds\n","\n","                dmulw = dadd * np.ones_like(mulw)\n","                dmulu = dadd * np.ones_like(mulu)\n","\n","                dW_i = np.dot(W, layers[t]['prev_s'])\n","                dprev_s = np.dot(np.transpose(W), dmulw)\n","\n","                new_input = np.zeros(x.shape)\n","                new_input[t] = x[t]\n","                dU_i = np.dot(U, new_input)\n","                dx = np.dot(np.transpose(U), dmulu)\n","\n","                dU_t += dU_i\n","                dW_t += dW_i\n","                \n","            dV += dV_t\n","            dU += dU_t\n","            dW += dW_t\n","\n","            # if dU.max() > max_clip_value:\n","            #     dU[dU > max_clip_value] = max_clip_value\n","            # if dV.max() > max_clip_value:\n","            #     dV[dV > max_clip_value] = max_clip_value\n","            # if dW.max() > max_clip_value:\n","            #     dW[dW > max_clip_value] = max_clip_value\n","                \n","            \n","            # if dU.min() < min_clip_value:\n","            #     dU[dU < min_clip_value] = min_clip_value\n","            # if dV.min() < min_clip_value:\n","            #     dV[dV < min_clip_value] = min_clip_value\n","            # if dW.min() < min_clip_value:\n","            #     dW[dW < min_clip_value] = min_clip_value\n","        \n","        # update\n","        U -= learning_rate * dU\n","        V -= learning_rate * dV\n","        W -= learning_rate * dW"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch:  1 , Loss:  [[11.47040665]]\n","Epoch:  2 , Loss:  [[11.47548062]]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:96: RuntimeWarning: overflow encountered in multiply\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: RuntimeWarning: overflow encountered in multiply\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch:  3 , Loss:  [[11.47781683]]\n","Epoch:  4 , Loss:  [[11.48001257]]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:114: RuntimeWarning: overflow encountered in add\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch:  5 , Loss:  [[nan]]\n","Epoch:  6 , Loss:  [[nan]]\n","Epoch:  7 , Loss:  [[nan]]\n","Epoch:  8 , Loss:  [[nan]]\n","Epoch:  9 , Loss:  [[nan]]\n","Epoch:  10 , Loss:  [[nan]]\n","Epoch:  11 , Loss:  [[nan]]\n","Epoch:  12 , Loss:  [[nan]]\n","Epoch:  13 , Loss:  [[nan]]\n","Epoch:  14 , Loss:  [[nan]]\n","Epoch:  15 , Loss:  [[nan]]\n","Epoch:  16 , Loss:  [[nan]]\n","Epoch:  17 , Loss:  [[nan]]\n","Epoch:  18 , Loss:  [[nan]]\n","Epoch:  19 , Loss:  [[nan]]\n","Epoch:  20 , Loss:  [[nan]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XHcC-ZvJmX6p"},"source":["preds_raw = []\n","preds = []\n","for i in range(Y.shape[0]):\n","    x, y = X[i], Y[i]\n","    prev_s = np.zeros((hidden_dim, 1))\n","    # Forward pass\n","    for t in range(T):\n","        mulu = np.dot(U, x)\n","        mulw = np.dot(W, prev_s)\n","        add = mulw + mulu\n","        s = sigmoid(add)\n","        mulv = sigmoid(np.dot(V, s))\n","        prev_s = s\n","\n","    preds_raw.append(mulv)\n","    preds.append(np.round(mulv))\n","    \n","preds = np.array(preds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XUrIxLGVmX3n","outputId":"7c142fe7-caf6-403f-fecb-c2ca8ed630f4"},"source":["preds_raw"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]]),\n"," array([[nan]])]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":32},"id":"kH58__FrmXzh","outputId":"aae5d8be-9709-4ae5-fd78-d4bcbdcce856"},"source":["result  = pd.DataFrame((Y.reshape(Y.shape[0],).tolist(),preds.reshape(preds.shape[0],).tolist())).T\n","result.columns  = (\"actual\", \"preds\")\n","conf_matrix  =  pd.crosstab(result.actual, result.preds)\n","conf_matrix"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: []\n","Index: []"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"NJSOH1-VkWXI"},"source":[""],"execution_count":null,"outputs":[]}]}