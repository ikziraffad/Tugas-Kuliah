{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Elman_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8-pNQYcFVwC"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Uzq07PySRnj"
      },
      "source": [
        "X = np.array([[2.7810836,2.550537003],\n",
        "\t[1.465489372,2.362125076],\n",
        "\t[3.396561688,4.400293529],\n",
        "\t[1.38807019,1.850220317],\n",
        "\t[3.06407232,3.005305973],\n",
        "\t[7.627531214,2.759262235],\n",
        "\t[5.332441248,2.088626775],\n",
        "\t[6.922596716,1.77106367],\n",
        "\t[8.675418651,-0.242068655],\n",
        "\t[7.673756466,3.508563011]])\n",
        "\n",
        "Y = np.array([0,0,0,0,0,1,1,1,1,1])\n",
        "\n",
        "X = X.reshape(X.shape[0], X.shape[1],1)\n",
        "Y = Y.reshape(Y.shape[0], 1)\n",
        "\n",
        "X_val=X\n",
        "Y_val=Y"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgsQvRf7UDBL",
        "outputId": "c59904c7-61ad-4606-9a3d-aa200ab756ed"
      },
      "source": [
        "print(X.shape, Y.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 2, 1) (10, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utubOHlSH_4F"
      },
      "source": [
        "#RNN Architecture\n",
        "\n",
        "learning_rate = 0.05\n",
        "nepoch = 500               \n",
        "T = 2                   # length of sequence\n",
        "hidden_dim = 10         \n",
        "output_dim = 1\n",
        "\n",
        "# bptt_truncate = 0\n",
        "# min_clip_value = -10\n",
        "# max_clip_value = 10"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDr3tOPOIJh6",
        "outputId": "0f0e8a0f-6603-4dbb-d80c-43d0e28223d7"
      },
      "source": [
        "#randomly initialize weights\n",
        "\n",
        "U = np.random.uniform(0, 1, (hidden_dim, T))\n",
        "W = np.random.uniform(0, 1, (hidden_dim, hidden_dim))\n",
        "V = np.random.uniform(0, 1, (output_dim, hidden_dim))\n",
        "print(U,V,W)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.72671086 0.0265279 ]\n",
            " [0.39711414 0.66179513]\n",
            " [0.89960486 0.82030839]\n",
            " [0.12833293 0.01927782]\n",
            " [0.38879306 0.47535484]\n",
            " [0.16887966 0.78985779]\n",
            " [0.00927101 0.92719376]\n",
            " [0.4170722  0.5742529 ]\n",
            " [0.18466165 0.72607017]\n",
            " [0.93470993 0.7891249 ]] [[0.66133693 0.77120423 0.79695688 0.68563813 0.21805199 0.74892262\n",
            "  0.1706146  0.0151264  0.64303345 0.06579108]] [[0.98329566 0.98557831 0.32609062 0.1120066  0.8294791  0.89507744\n",
            "  0.25658782 0.0517692  0.54561896 0.48531698]\n",
            " [0.67802044 0.54407174 0.87120095 0.06705982 0.14458609 0.2097538\n",
            "  0.14910333 0.05336298 0.87370592 0.32163788]\n",
            " [0.770716   0.27657976 0.1030236  0.37131522 0.65072275 0.30508242\n",
            "  0.09860316 0.36767594 0.46121393 0.68712096]\n",
            " [0.3544175  0.13329869 0.27861017 0.54255119 0.7908065  0.58265572\n",
            "  0.22036354 0.16028004 0.44997848 0.61886615]\n",
            " [0.09981604 0.9795927  0.87715666 0.39392445 0.58424869 0.22136064\n",
            "  0.65308954 0.03738507 0.03608689 0.98201116]\n",
            " [0.12997746 0.73165674 0.02039179 0.01558662 0.45652142 0.03694606\n",
            "  0.58692491 0.56711617 0.944366   0.58144781]\n",
            " [0.29973338 0.73692643 0.11690603 0.09291677 0.43828766 0.96724473\n",
            "  0.9692505  0.55307649 0.66871697 0.66506129]\n",
            " [0.40642913 0.60181676 0.87919382 0.42233552 0.80718856 0.89490497\n",
            "  0.17341964 0.1174196  0.27041127 0.78096083]\n",
            " [0.25013503 0.45731421 0.76794785 0.1397397  0.10471409 0.22092081\n",
            "  0.39043921 0.61364307 0.6080842  0.49084673]\n",
            " [0.21650221 0.3613952  0.0636917  0.6926474  0.79627088 0.3997486\n",
            "  0.43316873 0.31163651 0.93412949 0.59617829]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XfPGVZHIeeQ",
        "outputId": "8e5bcb66-f919-49ba-97c5-93e423a46a77"
      },
      "source": [
        "#forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "for epoch in range(nepoch):\n",
        "    # check loss on train\n",
        "    loss = 0.0\n",
        "    \n",
        "    # do a forward pass to get prediction\n",
        "    for i in range(Y.shape[0]):\n",
        "        x, y = X[i], Y[i]                    # get input, output values of each record\n",
        "        prev_s = np.zeros((hidden_dim, 1))   # here, prev-s is the value of the previous activation of hidden layer; which is initialized as all zeroes\n",
        "        for t in range(T):\n",
        "            new_input = np.zeros(x.shape)    # we then do a forward pass for every timestep in the sequence\n",
        "            new_input[t] = x[t]              # for this, we define a single input for that timestep\n",
        "            mulu = np.dot(U, new_input)\n",
        "            mulw = np.dot(W, prev_s)\n",
        "            add = mulw + mulu\n",
        "            s = sigmoid(add)\n",
        "            mulv = sigmoid(np.dot(V, s))\n",
        "            prev_s = s\n",
        "\n",
        "    # calculate error \n",
        "        loss_per_record = (y - mulv)**2 / 2\n",
        "        loss += loss_per_record\n",
        "    loss = loss / float(y.shape[0])\n",
        "\n",
        "    # # check loss on val\n",
        "    # val_loss = 0.0\n",
        "    # for i in range(Y_val.shape[0]):\n",
        "    #     x, y = X_val[i], Y_val[i]\n",
        "    #     prev_s = np.zeros((hidden_dim, 1))\n",
        "    #     for t in range(T):\n",
        "    #         new_input = np.zeros(x.shape)\n",
        "    #         new_input[t] = x[t]\n",
        "    #         mulu = np.dot(U, new_input)\n",
        "    #         mulw = np.dot(W, prev_s)\n",
        "    #         add = mulw + mulu\n",
        "    #         s = sigmoid(add)\n",
        "    #         mulv = sigmoid(np.dot(V, s))\n",
        "    #         prev_s = s\n",
        "\n",
        "    #     loss_per_record = (y - mulv)**2 / 2\n",
        "    #     val_loss += loss_per_record\n",
        "    # val_loss = val_loss / float(y.shape[0])\n",
        "\n",
        "    print('Epoch: ', epoch + 1, ', Loss: ', loss)\n",
        "\n",
        "    # train model\n",
        "    for i in range(Y.shape[0]):\n",
        "        x, y = X[i], Y[i]\n",
        "    \n",
        "        layers = []\n",
        "        prev_s = np.zeros((hidden_dim, 1))\n",
        "        dU = np.zeros(U.shape)\n",
        "        dV = np.zeros(V.shape)\n",
        "        dW = np.zeros(W.shape)\n",
        "        \n",
        "        dU_t = np.zeros(U.shape)\n",
        "        dV_t = np.zeros(V.shape)\n",
        "        dW_t = np.zeros(W.shape)\n",
        "        \n",
        "        dU_i = np.zeros(U.shape)\n",
        "        dW_i = np.zeros(W.shape)\n",
        "        \n",
        "        # forward pass\n",
        "        for t in range(T):\n",
        "            new_input = np.zeros(x.shape)\n",
        "            new_input[t] = x[t]\n",
        "            mulu = np.dot(U, new_input)\n",
        "            mulw = np.dot(W, prev_s)\n",
        "            add = mulw + mulu\n",
        "            s = sigmoid(add)\n",
        "            mulv = sigmoid(np.dot(V, s))\n",
        "            layers.append({'s':s, 'prev_s':prev_s})\n",
        "            prev_s = s\n",
        "\n",
        "        # derivative of pred\n",
        "        dmulv = (mulv - y)*(1-mulv)*(mulv)\n",
        "        \n",
        "        # backward pass\n",
        "        for t in range(T):\n",
        "            dV_t = np.dot(dmulv, np.transpose(layers[t]['s']))\n",
        "            dsv = np.dot(np.transpose(V), dmulv)\n",
        "            \n",
        "            ds = dsv\n",
        "            dadd = add * (1 - add) * ds\n",
        "            \n",
        "            dmulw = dadd * np.ones_like(mulw)\n",
        "\n",
        "            dprev_s = np.dot(np.transpose(W), dmulw)\n",
        "\n",
        "\n",
        "            for i in range(t, 0, -1):\n",
        "                ds = dsv + dprev_s\n",
        "                dadd = add * (1 - add) * ds\n",
        "\n",
        "                dmulw = dadd * np.ones_like(mulw)\n",
        "                dmulu = dadd * np.ones_like(mulu)\n",
        "\n",
        "                dW_i = np.dot(W, layers[t]['prev_s'])\n",
        "                dprev_s = np.dot(np.transpose(W), dmulw)\n",
        "\n",
        "                new_input = np.zeros(x.shape)\n",
        "                new_input[t] = x[t]\n",
        "                dU_i = np.dot(U, new_input)\n",
        "                dx = np.dot(np.transpose(U), dmulu)\n",
        "\n",
        "                dU_t += dU_i\n",
        "                dW_t += dW_i\n",
        "                \n",
        "            dV += dV_t\n",
        "            dU += dU_t\n",
        "            dW += dW_t\n",
        "\n",
        "            # if dU.max() > max_clip_value:\n",
        "            #     dU[dU > max_clip_value] = max_clip_value\n",
        "            # if dV.max() > max_clip_value:\n",
        "            #     dV[dV > max_clip_value] = max_clip_value\n",
        "            # if dW.max() > max_clip_value:\n",
        "            #     dW[dW > max_clip_value] = max_clip_value\n",
        "                \n",
        "            \n",
        "            # if dU.min() < min_clip_value:\n",
        "            #     dU[dU < min_clip_value] = min_clip_value\n",
        "            # if dV.min() < min_clip_value:\n",
        "            #     dV[dV < min_clip_value] = min_clip_value\n",
        "            # if dW.min() < min_clip_value:\n",
        "            #     dW[dW < min_clip_value] = min_clip_value\n",
        "        \n",
        "        # update\n",
        "        U -= learning_rate * dU\n",
        "        V -= learning_rate * dV\n",
        "        W -= learning_rate * dW"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1 , Loss:  [[1.34203838]]\n",
            "Epoch:  2 , Loss:  [[1.29616776]]\n",
            "Epoch:  3 , Loss:  [[1.2830914]]\n",
            "Epoch:  4 , Loss:  [[1.2789503]]\n",
            "Epoch:  5 , Loss:  [[1.27711353]]\n",
            "Epoch:  6 , Loss:  [[1.27592767]]\n",
            "Epoch:  7 , Loss:  [[1.27495726]]\n",
            "Epoch:  8 , Loss:  [[1.27408267]]\n",
            "Epoch:  9 , Loss:  [[1.27326892]]\n",
            "Epoch:  10 , Loss:  [[1.27250425]]\n",
            "Epoch:  11 , Loss:  [[1.2717834]]\n",
            "Epoch:  12 , Loss:  [[1.27110309]]\n",
            "Epoch:  13 , Loss:  [[1.27046068]]\n",
            "Epoch:  14 , Loss:  [[1.26985382]]\n",
            "Epoch:  15 , Loss:  [[1.26928037]]\n",
            "Epoch:  16 , Loss:  [[1.26873833]]\n",
            "Epoch:  17 , Loss:  [[1.2682258]]\n",
            "Epoch:  18 , Loss:  [[1.26774106]]\n",
            "Epoch:  19 , Loss:  [[1.26728244]]\n",
            "Epoch:  20 , Loss:  [[1.26684841]]\n",
            "Epoch:  21 , Loss:  [[1.26643753]]\n",
            "Epoch:  22 , Loss:  [[1.26604846]]\n",
            "Epoch:  23 , Loss:  [[1.26567991]]\n",
            "Epoch:  24 , Loss:  [[1.26533072]]\n",
            "Epoch:  25 , Loss:  [[1.26499976]]\n",
            "Epoch:  26 , Loss:  [[1.26468599]]\n",
            "Epoch:  27 , Loss:  [[1.26438844]]\n",
            "Epoch:  28 , Loss:  [[1.2641062]]\n",
            "Epoch:  29 , Loss:  [[1.26383839]]\n",
            "Epoch:  30 , Loss:  [[1.26358421]]\n",
            "Epoch:  31 , Loss:  [[1.26334291]]\n",
            "Epoch:  32 , Loss:  [[1.26311377]]\n",
            "Epoch:  33 , Loss:  [[1.26289613]]\n",
            "Epoch:  34 , Loss:  [[1.26268936]]\n",
            "Epoch:  35 , Loss:  [[1.26249286]]\n",
            "Epoch:  36 , Loss:  [[1.26230609]]\n",
            "Epoch:  37 , Loss:  [[1.26212852]]\n",
            "Epoch:  38 , Loss:  [[1.26195966]]\n",
            "Epoch:  39 , Loss:  [[1.26179905]]\n",
            "Epoch:  40 , Loss:  [[1.26164625]]\n",
            "Epoch:  41 , Loss:  [[1.26150086]]\n",
            "Epoch:  42 , Loss:  [[1.26136248]]\n",
            "Epoch:  43 , Loss:  [[1.26123076]]\n",
            "Epoch:  44 , Loss:  [[1.26110535]]\n",
            "Epoch:  45 , Loss:  [[1.26098593]]\n",
            "Epoch:  46 , Loss:  [[1.26087219]]\n",
            "Epoch:  47 , Loss:  [[1.26076385]]\n",
            "Epoch:  48 , Loss:  [[1.26066062]]\n",
            "Epoch:  49 , Loss:  [[1.26056227]]\n",
            "Epoch:  50 , Loss:  [[1.26046854]]\n",
            "Epoch:  51 , Loss:  [[1.2603792]]\n",
            "Epoch:  52 , Loss:  [[1.26029404]]\n",
            "Epoch:  53 , Loss:  [[1.26021285]]\n",
            "Epoch:  54 , Loss:  [[1.26013544]]\n",
            "Epoch:  55 , Loss:  [[1.26006163]]\n",
            "Epoch:  56 , Loss:  [[1.25999123]]\n",
            "Epoch:  57 , Loss:  [[1.25992409]]\n",
            "Epoch:  58 , Loss:  [[1.25986005]]\n",
            "Epoch:  59 , Loss:  [[1.25979896]]\n",
            "Epoch:  60 , Loss:  [[1.25974068]]\n",
            "Epoch:  61 , Loss:  [[1.25968508]]\n",
            "Epoch:  62 , Loss:  [[1.25963203]]\n",
            "Epoch:  63 , Loss:  [[1.25958141]]\n",
            "Epoch:  64 , Loss:  [[1.2595331]]\n",
            "Epoch:  65 , Loss:  [[1.25948701]]\n",
            "Epoch:  66 , Loss:  [[1.25944303]]\n",
            "Epoch:  67 , Loss:  [[1.25940105]]\n",
            "Epoch:  68 , Loss:  [[1.25936099]]\n",
            "Epoch:  69 , Loss:  [[1.25932276]]\n",
            "Epoch:  70 , Loss:  [[1.25928628]]\n",
            "Epoch:  71 , Loss:  [[1.25925146]]\n",
            "Epoch:  72 , Loss:  [[1.25921823]]\n",
            "Epoch:  73 , Loss:  [[1.25918651]]\n",
            "Epoch:  74 , Loss:  [[1.25915625]]\n",
            "Epoch:  75 , Loss:  [[1.25912737]]\n",
            "Epoch:  76 , Loss:  [[1.25909981]]\n",
            "Epoch:  77 , Loss:  [[1.25907351]]\n",
            "Epoch:  78 , Loss:  [[1.25904842]]\n",
            "Epoch:  79 , Loss:  [[1.25902448]]\n",
            "Epoch:  80 , Loss:  [[1.25900163]]\n",
            "Epoch:  81 , Loss:  [[1.25897985]]\n",
            "Epoch:  82 , Loss:  [[1.25895906]]\n",
            "Epoch:  83 , Loss:  [[1.25893924]]\n",
            "Epoch:  84 , Loss:  [[1.25892033]]\n",
            "Epoch:  85 , Loss:  [[1.2589023]]\n",
            "Epoch:  86 , Loss:  [[1.25888511]]\n",
            "Epoch:  87 , Loss:  [[1.25886872]]\n",
            "Epoch:  88 , Loss:  [[1.2588531]]\n",
            "Epoch:  89 , Loss:  [[1.25883822]]\n",
            "Epoch:  90 , Loss:  [[1.25882403]]\n",
            "Epoch:  91 , Loss:  [[1.25881052]]\n",
            "Epoch:  92 , Loss:  [[1.25879764]]\n",
            "Epoch:  93 , Loss:  [[1.25878538]]\n",
            "Epoch:  94 , Loss:  [[1.25877371]]\n",
            "Epoch:  95 , Loss:  [[1.2587626]]\n",
            "Epoch:  96 , Loss:  [[1.25875203]]\n",
            "Epoch:  97 , Loss:  [[1.25874197]]\n",
            "Epoch:  98 , Loss:  [[1.2587324]]\n",
            "Epoch:  99 , Loss:  [[1.25872331]]\n",
            "Epoch:  100 , Loss:  [[1.25871466]]\n",
            "Epoch:  101 , Loss:  [[1.25870645]]\n",
            "Epoch:  102 , Loss:  [[1.25869864]]\n",
            "Epoch:  103 , Loss:  [[1.25869124]]\n",
            "Epoch:  104 , Loss:  [[1.25868421]]\n",
            "Epoch:  105 , Loss:  [[1.25867754]]\n",
            "Epoch:  106 , Loss:  [[1.25867122]]\n",
            "Epoch:  107 , Loss:  [[1.25866523]]\n",
            "Epoch:  108 , Loss:  [[1.25865956]]\n",
            "Epoch:  109 , Loss:  [[1.2586542]]\n",
            "Epoch:  110 , Loss:  [[1.25864912]]\n",
            "Epoch:  111 , Loss:  [[1.25864432]]\n",
            "Epoch:  112 , Loss:  [[1.25863979]]\n",
            "Epoch:  113 , Loss:  [[1.25863551]]\n",
            "Epoch:  114 , Loss:  [[1.25863148]]\n",
            "Epoch:  115 , Loss:  [[1.25862769]]\n",
            "Epoch:  116 , Loss:  [[1.25862411]]\n",
            "Epoch:  117 , Loss:  [[1.25862075]]\n",
            "Epoch:  118 , Loss:  [[1.2586176]]\n",
            "Epoch:  119 , Loss:  [[1.25861464]]\n",
            "Epoch:  120 , Loss:  [[1.25861188]]\n",
            "Epoch:  121 , Loss:  [[1.25860929]]\n",
            "Epoch:  122 , Loss:  [[1.25860688]]\n",
            "Epoch:  123 , Loss:  [[1.25860463]]\n",
            "Epoch:  124 , Loss:  [[1.25860254]]\n",
            "Epoch:  125 , Loss:  [[1.2586006]]\n",
            "Epoch:  126 , Loss:  [[1.2585988]]\n",
            "Epoch:  127 , Loss:  [[1.25859715]]\n",
            "Epoch:  128 , Loss:  [[1.25859563]]\n",
            "Epoch:  129 , Loss:  [[1.25859424]]\n",
            "Epoch:  130 , Loss:  [[1.25859297]]\n",
            "Epoch:  131 , Loss:  [[1.25859181]]\n",
            "Epoch:  132 , Loss:  [[1.25859077]]\n",
            "Epoch:  133 , Loss:  [[1.25858984]]\n",
            "Epoch:  134 , Loss:  [[1.25858901]]\n",
            "Epoch:  135 , Loss:  [[1.25858828]]\n",
            "Epoch:  136 , Loss:  [[1.25858764]]\n",
            "Epoch:  137 , Loss:  [[1.2585871]]\n",
            "Epoch:  138 , Loss:  [[1.25858664]]\n",
            "Epoch:  139 , Loss:  [[1.25858626]]\n",
            "Epoch:  140 , Loss:  [[1.25858596]]\n",
            "Epoch:  141 , Loss:  [[1.25858574]]\n",
            "Epoch:  142 , Loss:  [[1.25858559]]\n",
            "Epoch:  143 , Loss:  [[1.25858552]]\n",
            "Epoch:  144 , Loss:  [[1.2585855]]\n",
            "Epoch:  145 , Loss:  [[1.25858556]]\n",
            "Epoch:  146 , Loss:  [[1.25858567]]\n",
            "Epoch:  147 , Loss:  [[1.25858585]]\n",
            "Epoch:  148 , Loss:  [[1.25858608]]\n",
            "Epoch:  149 , Loss:  [[1.25858636]]\n",
            "Epoch:  150 , Loss:  [[1.2585867]]\n",
            "Epoch:  151 , Loss:  [[1.25858708]]\n",
            "Epoch:  152 , Loss:  [[1.25858752]]\n",
            "Epoch:  153 , Loss:  [[1.258588]]\n",
            "Epoch:  154 , Loss:  [[1.25858852]]\n",
            "Epoch:  155 , Loss:  [[1.25858908]]\n",
            "Epoch:  156 , Loss:  [[1.25858969]]\n",
            "Epoch:  157 , Loss:  [[1.25859033]]\n",
            "Epoch:  158 , Loss:  [[1.25859101]]\n",
            "Epoch:  159 , Loss:  [[1.25859172]]\n",
            "Epoch:  160 , Loss:  [[1.25859247]]\n",
            "Epoch:  161 , Loss:  [[1.25859325]]\n",
            "Epoch:  162 , Loss:  [[1.25859407]]\n",
            "Epoch:  163 , Loss:  [[1.25859491]]\n",
            "Epoch:  164 , Loss:  [[1.25859578]]\n",
            "Epoch:  165 , Loss:  [[1.25859667]]\n",
            "Epoch:  166 , Loss:  [[1.2585976]]\n",
            "Epoch:  167 , Loss:  [[1.25859855]]\n",
            "Epoch:  168 , Loss:  [[1.25859952]]\n",
            "Epoch:  169 , Loss:  [[1.25860051]]\n",
            "Epoch:  170 , Loss:  [[1.25860153]]\n",
            "Epoch:  171 , Loss:  [[1.25860257]]\n",
            "Epoch:  172 , Loss:  [[1.25860363]]\n",
            "Epoch:  173 , Loss:  [[1.2586047]]\n",
            "Epoch:  174 , Loss:  [[1.2586058]]\n",
            "Epoch:  175 , Loss:  [[1.25860692]]\n",
            "Epoch:  176 , Loss:  [[1.25860805]]\n",
            "Epoch:  177 , Loss:  [[1.25860919]]\n",
            "Epoch:  178 , Loss:  [[1.25861036]]\n",
            "Epoch:  179 , Loss:  [[1.25861153]]\n",
            "Epoch:  180 , Loss:  [[1.25861273]]\n",
            "Epoch:  181 , Loss:  [[1.25861393]]\n",
            "Epoch:  182 , Loss:  [[1.25861515]]\n",
            "Epoch:  183 , Loss:  [[1.25861638]]\n",
            "Epoch:  184 , Loss:  [[1.25861763]]\n",
            "Epoch:  185 , Loss:  [[1.25861888]]\n",
            "Epoch:  186 , Loss:  [[1.25862015]]\n",
            "Epoch:  187 , Loss:  [[1.25862143]]\n",
            "Epoch:  188 , Loss:  [[1.25862271]]\n",
            "Epoch:  189 , Loss:  [[1.25862401]]\n",
            "Epoch:  190 , Loss:  [[1.25862532]]\n",
            "Epoch:  191 , Loss:  [[1.25862663]]\n",
            "Epoch:  192 , Loss:  [[1.25862796]]\n",
            "Epoch:  193 , Loss:  [[1.25862929]]\n",
            "Epoch:  194 , Loss:  [[1.25863063]]\n",
            "Epoch:  195 , Loss:  [[1.25863198]]\n",
            "Epoch:  196 , Loss:  [[1.25863333]]\n",
            "Epoch:  197 , Loss:  [[1.25863469]]\n",
            "Epoch:  198 , Loss:  [[1.25863606]]\n",
            "Epoch:  199 , Loss:  [[1.25863744]]\n",
            "Epoch:  200 , Loss:  [[1.25863882]]\n",
            "Epoch:  201 , Loss:  [[1.25864021]]\n",
            "Epoch:  202 , Loss:  [[1.2586416]]\n",
            "Epoch:  203 , Loss:  [[1.258643]]\n",
            "Epoch:  204 , Loss:  [[1.2586444]]\n",
            "Epoch:  205 , Loss:  [[1.25864581]]\n",
            "Epoch:  206 , Loss:  [[1.25864722]]\n",
            "Epoch:  207 , Loss:  [[1.25864864]]\n",
            "Epoch:  208 , Loss:  [[1.25865006]]\n",
            "Epoch:  209 , Loss:  [[1.25865148]]\n",
            "Epoch:  210 , Loss:  [[1.25865291]]\n",
            "Epoch:  211 , Loss:  [[1.25865434]]\n",
            "Epoch:  212 , Loss:  [[1.25865578]]\n",
            "Epoch:  213 , Loss:  [[1.25865722]]\n",
            "Epoch:  214 , Loss:  [[1.25865867]]\n",
            "Epoch:  215 , Loss:  [[1.25866011]]\n",
            "Epoch:  216 , Loss:  [[1.25866156]]\n",
            "Epoch:  217 , Loss:  [[1.25866302]]\n",
            "Epoch:  218 , Loss:  [[1.25866447]]\n",
            "Epoch:  219 , Loss:  [[1.25866593]]\n",
            "Epoch:  220 , Loss:  [[1.25866739]]\n",
            "Epoch:  221 , Loss:  [[1.25866885]]\n",
            "Epoch:  222 , Loss:  [[1.25867032]]\n",
            "Epoch:  223 , Loss:  [[1.25867179]]\n",
            "Epoch:  224 , Loss:  [[1.25867326]]\n",
            "Epoch:  225 , Loss:  [[1.25867473]]\n",
            "Epoch:  226 , Loss:  [[1.25867621]]\n",
            "Epoch:  227 , Loss:  [[1.25867768]]\n",
            "Epoch:  228 , Loss:  [[1.25867916]]\n",
            "Epoch:  229 , Loss:  [[1.25868064]]\n",
            "Epoch:  230 , Loss:  [[1.25868212]]\n",
            "Epoch:  231 , Loss:  [[1.25868361]]\n",
            "Epoch:  232 , Loss:  [[1.25868509]]\n",
            "Epoch:  233 , Loss:  [[1.25868658]]\n",
            "Epoch:  234 , Loss:  [[1.25868807]]\n",
            "Epoch:  235 , Loss:  [[1.25868956]]\n",
            "Epoch:  236 , Loss:  [[1.25869105]]\n",
            "Epoch:  237 , Loss:  [[1.25869254]]\n",
            "Epoch:  238 , Loss:  [[1.25869403]]\n",
            "Epoch:  239 , Loss:  [[1.25869552]]\n",
            "Epoch:  240 , Loss:  [[1.25869702]]\n",
            "Epoch:  241 , Loss:  [[1.25869852]]\n",
            "Epoch:  242 , Loss:  [[1.25870001]]\n",
            "Epoch:  243 , Loss:  [[1.25870151]]\n",
            "Epoch:  244 , Loss:  [[1.25870301]]\n",
            "Epoch:  245 , Loss:  [[1.25870451]]\n",
            "Epoch:  246 , Loss:  [[1.25870601]]\n",
            "Epoch:  247 , Loss:  [[1.25870751]]\n",
            "Epoch:  248 , Loss:  [[1.25870902]]\n",
            "Epoch:  249 , Loss:  [[1.25871052]]\n",
            "Epoch:  250 , Loss:  [[1.25871202]]\n",
            "Epoch:  251 , Loss:  [[1.25871353]]\n",
            "Epoch:  252 , Loss:  [[1.25871503]]\n",
            "Epoch:  253 , Loss:  [[1.25871654]]\n",
            "Epoch:  254 , Loss:  [[1.25871805]]\n",
            "Epoch:  255 , Loss:  [[1.25871955]]\n",
            "Epoch:  256 , Loss:  [[1.25872106]]\n",
            "Epoch:  257 , Loss:  [[1.25872257]]\n",
            "Epoch:  258 , Loss:  [[1.25872408]]\n",
            "Epoch:  259 , Loss:  [[1.25872559]]\n",
            "Epoch:  260 , Loss:  [[1.2587271]]\n",
            "Epoch:  261 , Loss:  [[1.25872861]]\n",
            "Epoch:  262 , Loss:  [[1.25873012]]\n",
            "Epoch:  263 , Loss:  [[1.25873163]]\n",
            "Epoch:  264 , Loss:  [[1.25873314]]\n",
            "Epoch:  265 , Loss:  [[1.25873465]]\n",
            "Epoch:  266 , Loss:  [[1.25873617]]\n",
            "Epoch:  267 , Loss:  [[1.25873768]]\n",
            "Epoch:  268 , Loss:  [[1.25873919]]\n",
            "Epoch:  269 , Loss:  [[1.2587407]]\n",
            "Epoch:  270 , Loss:  [[1.25874222]]\n",
            "Epoch:  271 , Loss:  [[1.25874373]]\n",
            "Epoch:  272 , Loss:  [[1.25874525]]\n",
            "Epoch:  273 , Loss:  [[1.25874676]]\n",
            "Epoch:  274 , Loss:  [[1.25874827]]\n",
            "Epoch:  275 , Loss:  [[1.25874979]]\n",
            "Epoch:  276 , Loss:  [[1.2587513]]\n",
            "Epoch:  277 , Loss:  [[1.25875282]]\n",
            "Epoch:  278 , Loss:  [[1.25875434]]\n",
            "Epoch:  279 , Loss:  [[1.25875585]]\n",
            "Epoch:  280 , Loss:  [[1.25875737]]\n",
            "Epoch:  281 , Loss:  [[1.25875888]]\n",
            "Epoch:  282 , Loss:  [[1.2587604]]\n",
            "Epoch:  283 , Loss:  [[1.25876192]]\n",
            "Epoch:  284 , Loss:  [[1.25876343]]\n",
            "Epoch:  285 , Loss:  [[1.25876495]]\n",
            "Epoch:  286 , Loss:  [[1.25876647]]\n",
            "Epoch:  287 , Loss:  [[1.25876798]]\n",
            "Epoch:  288 , Loss:  [[1.2587695]]\n",
            "Epoch:  289 , Loss:  [[1.25877102]]\n",
            "Epoch:  290 , Loss:  [[1.25877254]]\n",
            "Epoch:  291 , Loss:  [[1.25877405]]\n",
            "Epoch:  292 , Loss:  [[1.25877557]]\n",
            "Epoch:  293 , Loss:  [[1.25877709]]\n",
            "Epoch:  294 , Loss:  [[1.25877861]]\n",
            "Epoch:  295 , Loss:  [[1.25878013]]\n",
            "Epoch:  296 , Loss:  [[1.25878164]]\n",
            "Epoch:  297 , Loss:  [[1.25878316]]\n",
            "Epoch:  298 , Loss:  [[1.25878468]]\n",
            "Epoch:  299 , Loss:  [[1.2587862]]\n",
            "Epoch:  300 , Loss:  [[1.25878772]]\n",
            "Epoch:  301 , Loss:  [[1.25878924]]\n",
            "Epoch:  302 , Loss:  [[1.25879076]]\n",
            "Epoch:  303 , Loss:  [[1.25879228]]\n",
            "Epoch:  304 , Loss:  [[1.25879379]]\n",
            "Epoch:  305 , Loss:  [[1.25879531]]\n",
            "Epoch:  306 , Loss:  [[1.25879683]]\n",
            "Epoch:  307 , Loss:  [[1.25879835]]\n",
            "Epoch:  308 , Loss:  [[1.25879987]]\n",
            "Epoch:  309 , Loss:  [[1.25880139]]\n",
            "Epoch:  310 , Loss:  [[1.25880291]]\n",
            "Epoch:  311 , Loss:  [[1.25880443]]\n",
            "Epoch:  312 , Loss:  [[1.25880595]]\n",
            "Epoch:  313 , Loss:  [[1.25880747]]\n",
            "Epoch:  314 , Loss:  [[1.25880899]]\n",
            "Epoch:  315 , Loss:  [[1.25881051]]\n",
            "Epoch:  316 , Loss:  [[1.25881203]]\n",
            "Epoch:  317 , Loss:  [[1.25881355]]\n",
            "Epoch:  318 , Loss:  [[1.25881507]]\n",
            "Epoch:  319 , Loss:  [[1.25881659]]\n",
            "Epoch:  320 , Loss:  [[1.25881811]]\n",
            "Epoch:  321 , Loss:  [[1.25881963]]\n",
            "Epoch:  322 , Loss:  [[1.25882115]]\n",
            "Epoch:  323 , Loss:  [[1.25882267]]\n",
            "Epoch:  324 , Loss:  [[1.25882419]]\n",
            "Epoch:  325 , Loss:  [[1.25882571]]\n",
            "Epoch:  326 , Loss:  [[1.25882723]]\n",
            "Epoch:  327 , Loss:  [[1.25882875]]\n",
            "Epoch:  328 , Loss:  [[1.25883027]]\n",
            "Epoch:  329 , Loss:  [[1.25883179]]\n",
            "Epoch:  330 , Loss:  [[1.25883331]]\n",
            "Epoch:  331 , Loss:  [[1.25883483]]\n",
            "Epoch:  332 , Loss:  [[1.25883635]]\n",
            "Epoch:  333 , Loss:  [[1.25883787]]\n",
            "Epoch:  334 , Loss:  [[1.25883939]]\n",
            "Epoch:  335 , Loss:  [[1.25884091]]\n",
            "Epoch:  336 , Loss:  [[1.25884243]]\n",
            "Epoch:  337 , Loss:  [[1.25884395]]\n",
            "Epoch:  338 , Loss:  [[1.25884547]]\n",
            "Epoch:  339 , Loss:  [[1.25884699]]\n",
            "Epoch:  340 , Loss:  [[1.25884851]]\n",
            "Epoch:  341 , Loss:  [[1.25885003]]\n",
            "Epoch:  342 , Loss:  [[1.25885155]]\n",
            "Epoch:  343 , Loss:  [[1.25885307]]\n",
            "Epoch:  344 , Loss:  [[1.25885459]]\n",
            "Epoch:  345 , Loss:  [[1.25885611]]\n",
            "Epoch:  346 , Loss:  [[1.25885763]]\n",
            "Epoch:  347 , Loss:  [[1.25885915]]\n",
            "Epoch:  348 , Loss:  [[1.25886067]]\n",
            "Epoch:  349 , Loss:  [[1.25886219]]\n",
            "Epoch:  350 , Loss:  [[1.25886371]]\n",
            "Epoch:  351 , Loss:  [[1.25886523]]\n",
            "Epoch:  352 , Loss:  [[1.25886675]]\n",
            "Epoch:  353 , Loss:  [[1.25886827]]\n",
            "Epoch:  354 , Loss:  [[1.25886979]]\n",
            "Epoch:  355 , Loss:  [[1.25887132]]\n",
            "Epoch:  356 , Loss:  [[1.25887284]]\n",
            "Epoch:  357 , Loss:  [[1.25887436]]\n",
            "Epoch:  358 , Loss:  [[1.25887588]]\n",
            "Epoch:  359 , Loss:  [[1.2588774]]\n",
            "Epoch:  360 , Loss:  [[1.25887892]]\n",
            "Epoch:  361 , Loss:  [[1.25888044]]\n",
            "Epoch:  362 , Loss:  [[1.25888196]]\n",
            "Epoch:  363 , Loss:  [[1.25888348]]\n",
            "Epoch:  364 , Loss:  [[1.258885]]\n",
            "Epoch:  365 , Loss:  [[1.25888652]]\n",
            "Epoch:  366 , Loss:  [[1.25888804]]\n",
            "Epoch:  367 , Loss:  [[1.25888956]]\n",
            "Epoch:  368 , Loss:  [[1.25889108]]\n",
            "Epoch:  369 , Loss:  [[1.2588926]]\n",
            "Epoch:  370 , Loss:  [[1.25889412]]\n",
            "Epoch:  371 , Loss:  [[1.25889565]]\n",
            "Epoch:  372 , Loss:  [[1.25889717]]\n",
            "Epoch:  373 , Loss:  [[1.25889869]]\n",
            "Epoch:  374 , Loss:  [[1.25890021]]\n",
            "Epoch:  375 , Loss:  [[1.25890173]]\n",
            "Epoch:  376 , Loss:  [[1.25890325]]\n",
            "Epoch:  377 , Loss:  [[1.25890477]]\n",
            "Epoch:  378 , Loss:  [[1.25890629]]\n",
            "Epoch:  379 , Loss:  [[1.25890781]]\n",
            "Epoch:  380 , Loss:  [[1.25890933]]\n",
            "Epoch:  381 , Loss:  [[1.25891085]]\n",
            "Epoch:  382 , Loss:  [[1.25891237]]\n",
            "Epoch:  383 , Loss:  [[1.25891389]]\n",
            "Epoch:  384 , Loss:  [[1.25891541]]\n",
            "Epoch:  385 , Loss:  [[1.25891694]]\n",
            "Epoch:  386 , Loss:  [[1.25891846]]\n",
            "Epoch:  387 , Loss:  [[1.25891998]]\n",
            "Epoch:  388 , Loss:  [[1.2589215]]\n",
            "Epoch:  389 , Loss:  [[1.25892302]]\n",
            "Epoch:  390 , Loss:  [[1.25892454]]\n",
            "Epoch:  391 , Loss:  [[1.25892606]]\n",
            "Epoch:  392 , Loss:  [[1.25892758]]\n",
            "Epoch:  393 , Loss:  [[1.2589291]]\n",
            "Epoch:  394 , Loss:  [[1.25893062]]\n",
            "Epoch:  395 , Loss:  [[1.25893214]]\n",
            "Epoch:  396 , Loss:  [[1.25893366]]\n",
            "Epoch:  397 , Loss:  [[1.25893518]]\n",
            "Epoch:  398 , Loss:  [[1.2589367]]\n",
            "Epoch:  399 , Loss:  [[1.25893823]]\n",
            "Epoch:  400 , Loss:  [[1.25893975]]\n",
            "Epoch:  401 , Loss:  [[1.25894127]]\n",
            "Epoch:  402 , Loss:  [[1.25894279]]\n",
            "Epoch:  403 , Loss:  [[1.25894431]]\n",
            "Epoch:  404 , Loss:  [[1.25894583]]\n",
            "Epoch:  405 , Loss:  [[1.25894735]]\n",
            "Epoch:  406 , Loss:  [[1.25894887]]\n",
            "Epoch:  407 , Loss:  [[1.25895039]]\n",
            "Epoch:  408 , Loss:  [[1.25895191]]\n",
            "Epoch:  409 , Loss:  [[1.25895343]]\n",
            "Epoch:  410 , Loss:  [[1.25895495]]\n",
            "Epoch:  411 , Loss:  [[1.25895648]]\n",
            "Epoch:  412 , Loss:  [[1.258958]]\n",
            "Epoch:  413 , Loss:  [[1.25895952]]\n",
            "Epoch:  414 , Loss:  [[1.25896104]]\n",
            "Epoch:  415 , Loss:  [[1.25896256]]\n",
            "Epoch:  416 , Loss:  [[1.25896408]]\n",
            "Epoch:  417 , Loss:  [[1.2589656]]\n",
            "Epoch:  418 , Loss:  [[1.25896712]]\n",
            "Epoch:  419 , Loss:  [[1.25896864]]\n",
            "Epoch:  420 , Loss:  [[1.25897016]]\n",
            "Epoch:  421 , Loss:  [[1.25897168]]\n",
            "Epoch:  422 , Loss:  [[1.2589732]]\n",
            "Epoch:  423 , Loss:  [[1.25897473]]\n",
            "Epoch:  424 , Loss:  [[1.25897625]]\n",
            "Epoch:  425 , Loss:  [[1.25897777]]\n",
            "Epoch:  426 , Loss:  [[1.25897929]]\n",
            "Epoch:  427 , Loss:  [[1.25898081]]\n",
            "Epoch:  428 , Loss:  [[1.25898233]]\n",
            "Epoch:  429 , Loss:  [[1.25898385]]\n",
            "Epoch:  430 , Loss:  [[1.25898537]]\n",
            "Epoch:  431 , Loss:  [[1.25898689]]\n",
            "Epoch:  432 , Loss:  [[1.25898841]]\n",
            "Epoch:  433 , Loss:  [[1.25898993]]\n",
            "Epoch:  434 , Loss:  [[1.25899146]]\n",
            "Epoch:  435 , Loss:  [[1.25899298]]\n",
            "Epoch:  436 , Loss:  [[1.2589945]]\n",
            "Epoch:  437 , Loss:  [[1.25899602]]\n",
            "Epoch:  438 , Loss:  [[1.25899754]]\n",
            "Epoch:  439 , Loss:  [[1.25899906]]\n",
            "Epoch:  440 , Loss:  [[1.25900058]]\n",
            "Epoch:  441 , Loss:  [[1.2590021]]\n",
            "Epoch:  442 , Loss:  [[1.25900362]]\n",
            "Epoch:  443 , Loss:  [[1.25900514]]\n",
            "Epoch:  444 , Loss:  [[1.25900666]]\n",
            "Epoch:  445 , Loss:  [[1.25900818]]\n",
            "Epoch:  446 , Loss:  [[1.25900971]]\n",
            "Epoch:  447 , Loss:  [[1.25901123]]\n",
            "Epoch:  448 , Loss:  [[1.25901275]]\n",
            "Epoch:  449 , Loss:  [[1.25901427]]\n",
            "Epoch:  450 , Loss:  [[1.25901579]]\n",
            "Epoch:  451 , Loss:  [[1.25901731]]\n",
            "Epoch:  452 , Loss:  [[1.25901883]]\n",
            "Epoch:  453 , Loss:  [[1.25902035]]\n",
            "Epoch:  454 , Loss:  [[1.25902187]]\n",
            "Epoch:  455 , Loss:  [[1.25902339]]\n",
            "Epoch:  456 , Loss:  [[1.25902492]]\n",
            "Epoch:  457 , Loss:  [[1.25902644]]\n",
            "Epoch:  458 , Loss:  [[1.25902796]]\n",
            "Epoch:  459 , Loss:  [[1.25902948]]\n",
            "Epoch:  460 , Loss:  [[1.259031]]\n",
            "Epoch:  461 , Loss:  [[1.25903252]]\n",
            "Epoch:  462 , Loss:  [[1.25903404]]\n",
            "Epoch:  463 , Loss:  [[1.25903556]]\n",
            "Epoch:  464 , Loss:  [[1.25903708]]\n",
            "Epoch:  465 , Loss:  [[1.2590386]]\n",
            "Epoch:  466 , Loss:  [[1.25904012]]\n",
            "Epoch:  467 , Loss:  [[1.25904165]]\n",
            "Epoch:  468 , Loss:  [[1.25904317]]\n",
            "Epoch:  469 , Loss:  [[1.25904469]]\n",
            "Epoch:  470 , Loss:  [[1.25904621]]\n",
            "Epoch:  471 , Loss:  [[1.25904773]]\n",
            "Epoch:  472 , Loss:  [[1.25904925]]\n",
            "Epoch:  473 , Loss:  [[1.25905077]]\n",
            "Epoch:  474 , Loss:  [[1.25905229]]\n",
            "Epoch:  475 , Loss:  [[1.25905381]]\n",
            "Epoch:  476 , Loss:  [[1.25905533]]\n",
            "Epoch:  477 , Loss:  [[1.25905686]]\n",
            "Epoch:  478 , Loss:  [[1.25905838]]\n",
            "Epoch:  479 , Loss:  [[1.2590599]]\n",
            "Epoch:  480 , Loss:  [[1.25906142]]\n",
            "Epoch:  481 , Loss:  [[1.25906294]]\n",
            "Epoch:  482 , Loss:  [[1.25906446]]\n",
            "Epoch:  483 , Loss:  [[1.25906598]]\n",
            "Epoch:  484 , Loss:  [[1.2590675]]\n",
            "Epoch:  485 , Loss:  [[1.25906902]]\n",
            "Epoch:  486 , Loss:  [[1.25907054]]\n",
            "Epoch:  487 , Loss:  [[1.25907207]]\n",
            "Epoch:  488 , Loss:  [[1.25907359]]\n",
            "Epoch:  489 , Loss:  [[1.25907511]]\n",
            "Epoch:  490 , Loss:  [[1.25907663]]\n",
            "Epoch:  491 , Loss:  [[1.25907815]]\n",
            "Epoch:  492 , Loss:  [[1.25907967]]\n",
            "Epoch:  493 , Loss:  [[1.25908119]]\n",
            "Epoch:  494 , Loss:  [[1.25908271]]\n",
            "Epoch:  495 , Loss:  [[1.25908423]]\n",
            "Epoch:  496 , Loss:  [[1.25908575]]\n",
            "Epoch:  497 , Loss:  [[1.25908728]]\n",
            "Epoch:  498 , Loss:  [[1.2590888]]\n",
            "Epoch:  499 , Loss:  [[1.25909032]]\n",
            "Epoch:  500 , Loss:  [[1.25909184]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV-iwQzTMNK2"
      },
      "source": [
        "preds_raw = []\n",
        "preds = []\n",
        "for i in range(Y.shape[0]):\n",
        "    x, y = X[i], Y[i]\n",
        "    prev_s = np.zeros((hidden_dim, 1))\n",
        "    # Forward pass\n",
        "    for t in range(T):\n",
        "        mulu = np.dot(U, x)\n",
        "        mulw = np.dot(W, prev_s)\n",
        "        add = mulw + mulu\n",
        "        s = sigmoid(add)\n",
        "        mulv = sigmoid(np.dot(V, s))\n",
        "        prev_s = s\n",
        "\n",
        "    preds_raw.append(mulv)\n",
        "    preds.append(np.round(mulv))\n",
        "    \n",
        "preds = np.array(preds)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTq5pF1sVlkK",
        "outputId": "3061e7b4-42d3-43ce-bf30-d2e67bab76a7"
      },
      "source": [
        "preds_raw"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.99150841]]),\n",
              " array([[0.99141356]]),\n",
              " array([[0.99156588]]),\n",
              " array([[0.99132975]]),\n",
              " array([[0.99153487]]),\n",
              " array([[0.99159781]]),\n",
              " array([[0.99155818]]),\n",
              " array([[0.99157423]]),\n",
              " array([[0.99148384]]),\n",
              " array([[0.99160367]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "0Lf6wqA-MQPP",
        "outputId": "1096e2b0-26da-49b0-abcd-f0e99fba5071"
      },
      "source": [
        "result  = pd.DataFrame((Y.reshape(Y.shape[0],).tolist(),preds.reshape(preds.shape[0],).tolist())).T\n",
        "result.columns  = (\"actual\", \"preds\")\n",
        "conf_matrix  =  pd.crosstab(result.actual, result.preds)\n",
        "conf_matrix"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>preds</th>\n",
              "      <th>1.0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>actual</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "preds   1.0\n",
              "actual     \n",
              "0.0       5\n",
              "1.0       5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbtaYyFuNP1e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}